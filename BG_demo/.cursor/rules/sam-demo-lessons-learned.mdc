---
alwaysApply: true
---

# SAM Demo - Lessons Learned & Critical Discoveries

**Date**: December 2024  
**Status**: Complete Development Cycle Lessons  
**Purpose**: Capture all mistakes, optimizations, and discoveries to prevent future rework

---

## üö® **CRITICAL DISCOVERIES (DO NOT LOSE)**

### **1. Agent Search Service Configuration (MOST COMMON FAILURE)**
**Problem**: Agents report "document indexing appears to be misconfigured"
**Root Cause**: Missing ID Column and Title Column configuration in agent tools
**Solution**: ALWAYS configure:
- **ID Column**: `DOC_ID`
- **Title Column**: `FILE_URL`

**Technical Requirement**: Search services MUST include DOC_ID and FILE_URL in ATTRIBUTES:
```sql
ATTRIBUTES DOC_ID, FILE_URL, COMPANY_NAME, DOCUMENT_TYPE, DOC_DATE, AUTHOR
```

**Impact**: Without this, agents cannot generate citations or hyperlinks to source documents.

### **2. SQL-Based Bulk Document Generation (PERFORMANCE BREAKTHROUGH)**
**Discovery**: Individual Python AI_COMPLETE calls are extremely slow
**Problem**: 15+ minutes for 30 documents (unacceptable for demos)
**Solution**: SQL-based bulk generation using model grouping
**Result**: **90 seconds for 30 documents (10x improvement)**

**Technical Implementation**:
1. Pre-render all prompts to RENDERED_PROMPTS table
2. Use SQL `SNOWFLAKE.CORTEX.COMPLETE` with model grouping
3. Leverage Snowflake parallel processing
4. Robust fallback to individual generation if needed

**Key Functions**:
- `render_all_prompts()` - Fast prompt rendering (no AI calls)
- `store_rendered_prompts_to_table()` - Bulk prompt storage  
- `bulk_generate_documents_with_sql()` - Optimized SQL generation

### **3. Semantic View RELATIONSHIPS Syntax (CRITICAL)**
**Problem**: Semantic views without proper RELATIONSHIPS syntax fail
**Solution**: MUST use explicit RELATIONSHIPS configuration:
```sql
RELATIONSHIPS (
  client_crm_to_client_portfolios AS 
    client_portfolios (client_id) REFERENCES client_crm
)
```

**Impact**: Without this, cross-table queries in semantic views don't work properly.

---

## üîß **TECHNICAL LESSONS LEARNED**

### **Database Schema Design**
**‚úÖ What Works**:
- Foreign key constraints for semantic view RELATIONSHIPS
- Change tracking enabled for Cortex Search compatibility
- VARCHAR(16MB) for DOCUMENTS.CONTENT (handles large generated content)
- ARRAY data type for TAGS (properly handled by SQL generation)

**‚ùå Mistakes Made**:
- Initially missing DOC_ID in search service ATTRIBUTES
- Forgot change tracking requirement for DOCUMENTS table
- Originally used smaller VARCHAR sizes that truncated content

### **Document Generation Optimization**
**‚úÖ Breakthrough Discovery**:
- SQL-based bulk generation is 10x faster than individual Python calls
- Model grouping overcomes `SNOWFLAKE.CORTEX.COMPLETE` string literal requirement
- RENDERED_PROMPTS table enables efficient bulk processing

**‚ùå Initial Approach**:
- Row-by-row Python generation (extremely slow)
- No prompt pre-rendering (inefficient)
- No model grouping (couldn't use bulk SQL)

### **Search Service Configuration**
**‚úÖ Critical Requirements**:
- DOC_ID and FILE_URL MUST be in ATTRIBUTES section
- TARGET_LAG = '1 hour' for responsive indexing
- Change tracking enabled on source table
- Proper document type filtering for each service

**‚ùå Common Mistakes**:
- Missing DOC_ID in ATTRIBUTES (breaks agent citations)
- Not waiting for indexing completion (2-3 minutes)
- Wrong document type filters (agents can't find content)

---

## üìã **ARCHITECTURAL LESSONS**

### **Modular Design Success**
**‚úÖ What Works**:
- Sequential SQL file execution (clear dependencies)
- Separation of SQL schema from Python logic
- Single entry point orchestrator pattern
- Comprehensive error handling and progress reporting

**‚ùå Previous Approaches**:
- Monolithic setup scripts (hard to debug)
- Mixed SQL/Python in single files (maintainability issues)
- No clear execution order (dependency confusion)

### **Agent Configuration**
**‚úÖ Key Discoveries**:
- Sequential demo prompts work better than complex single prompts
- SAM philosophy must be naturally integrated (not forced)
- Framework scaffolding (10-Question) provides excellent structure
- Corporate Memory creates inimitable competitive advantage

**‚ùå Initial Mistakes**:
- Complex single prompts were hard to demo effectively
- Generic AI responses without SAM philosophy integration
- Missing citation requirements in agent instructions

---

## üéØ **DEMO DESIGN LESSONS**

### **Content Strategy**
**‚úÖ What Works**:
- Professional-grade synthetic content (800-1600 words)
- Period-appropriate historical documents (authentic voice)
- Specific company names (Arkadia Commerce vs generic names)
- Rich financial data (8 quarters minimum for realistic analysis)

**‚ùå Initial Approaches**:
- Generic company names and placeholder content
- Insufficient financial data depth
- Modern language in historical documents (broke authenticity)

### **Scenario Structure**
**‚úÖ Effective Approach**:
- Sequential prompts building on each other
- Clear competitive differentiation vs Bloomberg/FactSet
- Corporate Memory as unique selling proposition
- SAM philosophy consistently integrated

**‚ùå Less Effective**:
- Single complex prompts (hard to follow)
- Generic AI capabilities (no differentiation)
- Missing long-term investment perspective

---

## üõ†Ô∏è **TROUBLESHOOTING PLAYBOOK**

### **Most Common Issues & Quick Fixes**
1. **Agent says "document indexing misconfigured"**
   - Fix: Add ID Column (DOC_ID) and Title Column (FILE_URL) to all search service tools

2. **Search returns no results**
   - Fix: Wait 2-3 minutes for indexing, verify documents exist in source table

3. **Document generation very slow**
   - Fix: Ensure using `bulk_generate_documents_with_sql()` not individual calls

4. **Semantic view errors**
   - Fix: Verify RELATIONSHIPS syntax and foreign key constraints

5. **Agent can't access tools**
   - Fix: Verify database/schema context (FSI_DEMOS.SAM_DEMO)

### **Diagnostic Tools**
- `setup/diagnose_corporate_memory.sql` - Comprehensive troubleshooting
- `SEARCH_PREVIEW` function - Test search services directly
- `setup/final_verification.py` - Infrastructure verification

---

## üèÜ **SUCCESS PATTERNS**

### **What Made the Demo Successful**
1. **10x Performance Optimization** - SQL bulk generation breakthrough
2. **Corporate Memory Advantage** - 5+ years of historical context
3. **SAM Philosophy Integration** - Authentic investment terminology
4. **Sequential Demo Flow** - Building complexity over 3 scenarios
5. **Professional Content Quality** - Enterprise-grade synthetic documents

### **Competitive Advantages Achieved**
- **vs Bloomberg**: Conversational interface + corporate memory
- **vs FactSet**: SAM philosophy + framework methodology + thesis evolution
- **vs Generic AI**: Investment expertise + institutional knowledge

---

## üöÄ **FUTURE DEVELOPMENT GUIDELINES**

### **When Adding New Features**
1. Always use unified orchestrator pattern
2. Add comprehensive error handling and verification
3. Test with sequential demo prompts
4. Ensure SAM philosophy integration
5. Verify agent tool configuration (ID/Title columns)

### **Performance Considerations**
1. Use SQL-based bulk operations when possible
2. Leverage Snowflake parallel processing
3. Pre-render prompts for AI generation
4. Group operations by model type

### **Quality Standards**
1. Professional-grade content (1000+ words)
2. Authentic investment terminology
3. Period-appropriate historical voice
4. Minimum 3-4 citations per response
5. Clear competitive differentiation

---

This lessons learned document represents the complete knowledge gained from building a production-ready Snowflake Intelligence demo for asset management. Future development should reference these lessons to avoid repeating mistakes and build upon successful patterns.